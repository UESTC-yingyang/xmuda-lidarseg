2023-03-06 22:23:52,233 xmuda INFO: 1 GPUs available
2023-03-06 22:23:52,233 xmuda INFO: Namespace(config_file='configs/nuscenes/usa_singapore/xmuda.yaml', opts=[])
2023-03-06 22:23:52,233 xmuda INFO: Loaded configuration file configs/nuscenes/usa_singapore/xmuda.yaml
2023-03-06 22:23:52,233 xmuda INFO: Running with config:
AUTO_RESUME: True
DATALOADER:
  DROP_LAST: True
  NUM_WORKERS: 4
DATASET_SOURCE:
  NuScenesSCN:
    augmentation:
      color_jitter: (0.4, 0.4, 0.4)
      flip_x: 0.5
      fliplr: 0.5
      noisy_rot: 0.1
      rot_z: 6.2831
      transl: True
    full_scale: 4096
    image_normalizer: ()
    merge_classes: True
    nuscenes_dir: /media/lxf/Data/nuScenes
    preprocess_dir: /home/lxf/Workspace/Science/xMUDA/xmuda/xmuda/data/nuscenes/preprocess
    resize: (400, 225)
    scale: 20
    use_image: True
  TRAIN: ('train_usa',)
  TYPE: NuScenesSCN
DATASET_TARGET:
  NuScenesSCN:
    augmentation:
      color_jitter: (0.4, 0.4, 0.4)
      flip_x: 0.5
      fliplr: 0.5
      noisy_rot: 0.1
      rot_z: 6.2831
      transl: True
    full_scale: 4096
    image_normalizer: ()
    merge_classes: True
    nuscenes_dir: /media/lxf/Data/nuScenes
    preprocess_dir: /home/lxf/Workspace/Science/xMUDA/xmuda/xmuda/data/nuscenes/preprocess
    pselab_paths: ()
    resize: (400, 225)
    scale: 20
    use_image: True
  TEST: ('test_singapore',)
  TRAIN: ('train_singapore',)
  TYPE: NuScenesSCN
  VAL: ('val_singapore',)
MODEL:
  TYPE: 
MODEL_2D:
  CKPT_PATH: 
  DUAL_HEAD: True
  NUM_CLASSES: 5
  TYPE: UNetResNet34
  UNetResNet34:
    pretrained: True
MODEL_3D:
  CKPT_PATH: 
  DUAL_HEAD: True
  NUM_CLASSES: 5
  SCN:
    block_reps: 1
    full_scale: 4096
    in_channels: 1
    m: 16
    num_planes: 7
    residual_blocks: False
  TYPE: SCN
OPTIMIZER:
  Adam:
    betas: (0.9, 0.999)
  BASE_LR: 0.001
  TYPE: Adam
  WEIGHT_DECAY: 0.0
OUTPUT_DIR: outputs/nuscenes/usa_singapore/xmuda
RESUME_PATH: 
RESUME_STATES: True
RNG_SEED: 1
SCHEDULER:
  CLIP_LR: 0.0
  MAX_ITERATION: 100000
  MultiStepLR:
    gamma: 0.1
    milestones: (80000, 90000)
  TYPE: MultiStepLR
TRAIN:
  BATCH_SIZE: 8
  CHECKPOINT_PERIOD: 5000
  CLASS_WEIGHTS: [2.47956584, 4.26788384, 5.71114131, 3.80241668, 1.0]
  FROZEN_PATTERNS: ()
  LOG_PERIOD: 50
  MAX_TO_KEEP: 100
  SUMMARY_PERIOD: 50
  XMUDA:
    lambda_logcoral: 0.0
    lambda_minent: 0.0
    lambda_pl: 0.0
    lambda_xm_src: 1.0
    lambda_xm_trg: 0.1
VAL:
  BATCH_SIZE: 32
  LOG_PERIOD: 20
  METRIC: seg_iou
  PERIOD: 5000
2023-03-06 22:23:52,473 xmuda.train INFO: Build 2D model:
Net2DSeg(
  (net_2d): UNetResNet34(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (dec_t_conv_stage5): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (dec_conv_stage4): Sequential(
      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (dec_t_conv_stage4): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (dec_conv_stage3): Sequential(
      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (dec_t_conv_stage3): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (dec_conv_stage2): Sequential(
      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (dec_t_conv_stage2): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (dec_conv_stage1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (dropout): Dropout(p=0.4, inplace=False)
  )
  (linear): Linear(in_features=64, out_features=5, bias=True)
  (linear2): Linear(in_features=64, out_features=5, bias=True)
)
2023-03-06 22:23:52,484 xmuda.train INFO: Build 3D model:
Net3DSeg(
  (net_3d): UNetSCN(
    (sparseModel): Sequential(
      (0): InputLayer()
      (1): SubmanifoldConvolution 1->16 C3
      (2): Sequential(
        (0): Sequential(
          (0): BatchNormLeakyReLU(16,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
          (1): SubmanifoldConvolution 16->16 C3
        )
        (1): ConcatTable(
          (0): Identity()
          (1): Sequential(
            (0): BatchNormLeakyReLU(16,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
            (1): Convolution 16->32 C2/2
            (2): Sequential(
              (0): Sequential(
                (0): BatchNormLeakyReLU(32,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                (1): SubmanifoldConvolution 32->32 C3
              )
              (1): ConcatTable(
                (0): Identity()
                (1): Sequential(
                  (0): BatchNormLeakyReLU(32,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                  (1): Convolution 32->48 C2/2
                  (2): Sequential(
                    (0): Sequential(
                      (0): BatchNormLeakyReLU(48,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                      (1): SubmanifoldConvolution 48->48 C3
                    )
                    (1): ConcatTable(
                      (0): Identity()
                      (1): Sequential(
                        (0): BatchNormLeakyReLU(48,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                        (1): Convolution 48->64 C2/2
                        (2): Sequential(
                          (0): Sequential(
                            (0): BatchNormLeakyReLU(64,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                            (1): SubmanifoldConvolution 64->64 C3
                          )
                          (1): ConcatTable(
                            (0): Identity()
                            (1): Sequential(
                              (0): BatchNormLeakyReLU(64,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                              (1): Convolution 64->80 C2/2
                              (2): Sequential(
                                (0): Sequential(
                                  (0): BatchNormLeakyReLU(80,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                  (1): SubmanifoldConvolution 80->80 C3
                                )
                                (1): ConcatTable(
                                  (0): Identity()
                                  (1): Sequential(
                                    (0): BatchNormLeakyReLU(80,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                    (1): Convolution 80->96 C2/2
                                    (2): Sequential(
                                      (0): Sequential(
                                        (0): BatchNormLeakyReLU(96,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                        (1): SubmanifoldConvolution 96->96 C3
                                      )
                                      (1): ConcatTable(
                                        (0): Identity()
                                        (1): Sequential(
                                          (0): BatchNormLeakyReLU(96,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                          (1): Convolution 96->112 C2/2
                                          (2): Sequential(
                                            (0): Sequential(
                                              (0): BatchNormLeakyReLU(112,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                              (1): SubmanifoldConvolution 112->112 C3
                                            )
                                          )
                                          (3): BatchNormLeakyReLU(112,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                          (4): Deconvolution 112->96 C2/2
                                        )
                                      )
                                      (2): JoinTable()
                                      (3): Sequential(
                                        (0): BatchNormLeakyReLU(192,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                        (1): SubmanifoldConvolution 192->96 C3
                                      )
                                    )
                                    (3): BatchNormLeakyReLU(96,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                    (4): Deconvolution 96->80 C2/2
                                  )
                                )
                                (2): JoinTable()
                                (3): Sequential(
                                  (0): BatchNormLeakyReLU(160,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                                  (1): SubmanifoldConvolution 160->80 C3
                                )
                              )
                              (3): BatchNormLeakyReLU(80,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                              (4): Deconvolution 80->64 C2/2
                            )
                          )
                          (2): JoinTable()
                          (3): Sequential(
                            (0): BatchNormLeakyReLU(128,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                            (1): SubmanifoldConvolution 128->64 C3
                          )
                        )
                        (3): BatchNormLeakyReLU(64,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                        (4): Deconvolution 64->48 C2/2
                      )
                    )
                    (2): JoinTable()
                    (3): Sequential(
                      (0): BatchNormLeakyReLU(96,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                      (1): SubmanifoldConvolution 96->48 C3
                    )
                  )
                  (3): BatchNormLeakyReLU(48,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                  (4): Deconvolution 48->32 C2/2
                )
              )
              (2): JoinTable()
              (3): Sequential(
                (0): BatchNormLeakyReLU(64,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
                (1): SubmanifoldConvolution 64->32 C3
              )
            )
            (3): BatchNormLeakyReLU(32,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
            (4): Deconvolution 32->16 C2/2
          )
        )
        (2): JoinTable()
        (3): Sequential(
          (0): BatchNormLeakyReLU(32,eps=0.0001,momentum=0.99,affine=True,leakiness=0)
          (1): SubmanifoldConvolution 32->16 C3
        )
      )
      (3): BatchNormReLU(16,eps=0.0001,momentum=0.99,affine=True)
      (4): OutputLayer()
    )
  )
  (linear): Linear(in_features=16, out_features=5, bias=True)
  (linear2): Linear(in_features=16, out_features=5, bias=True)
)
2023-03-06 22:23:54,198 xmuda.train INFO: No checkpoint found. Initializing model from scratch
2023-03-06 22:23:54,198 xmuda.train INFO: No checkpoint found. Initializing model from scratch
2023-03-06 22:23:54,557 xmuda.train INFO: Start training from iteration 0
2023-03-06 22:23:56,208 xmuda.train INFO: iter:    1  seg_iou_2d: 0.1175  seg_iou_3d: 0.0459  seg_loss_src_2d: 1.4912 (1.4912)  seg_loss_src_3d: 1.4790 (1.4790)  xm_loss_src_2d: 0.0784 (0.0784)  xm_loss_src_3d: 0.0799 (0.0799)  xm_loss_trg_2d: 0.0800 (0.0800)  xm_loss_trg_3d: 0.0813 (0.0813)  time: 1.6499 (1.6499)  data: 0.4975 (0.4975)  lr: 1.00e-03  max mem: 4656
